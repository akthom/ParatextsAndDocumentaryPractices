<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id><journal-id journal-id-type="publisher-id">bioinformatics</journal-id><journal-id journal-id-type="hwp">bioinfo</journal-id><journal-title-group><journal-title>Bioinformatics</journal-title></journal-title-group><issn pub-type="ppub">1367-4803</issn><issn pub-type="epub">1460-2059</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">18586737</article-id><article-id pub-id-type="pmc">2718663</article-id><article-id pub-id-type="doi">10.1093/bioinformatics/btn188</article-id><article-id pub-id-type="publisher-id">btn188</article-id><article-categories><subj-group subj-group-type="heading"><subject>Ismb 2008 Conference Proceedings 19&#x02013;23 July 2008, Toronto</subject><subj-group><subject>Original Papers</subject><subj-group><subject>Other Bioinformatics Applications and Methods</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Classification of arrayCGH data using fused SVM</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rapaport</surname><given-names>Franck</given-names></name><xref ref-type="aff" rid="AFF1"><sup>1</sup></xref><xref ref-type="aff" rid="AFF1"><sup>2</sup></xref><xref ref-type="aff" rid="AFF1"><sup>3</sup></xref><xref ref-type="corresp" rid="COR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Barillot</surname><given-names>Emmanuel</given-names></name><xref ref-type="aff" rid="AFF1"><sup>1</sup></xref><xref ref-type="aff" rid="AFF1"><sup>2</sup></xref><xref ref-type="aff" rid="AFF1"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Vert</surname><given-names>Jean-Philippe</given-names></name><xref ref-type="aff" rid="AFF1"><sup>1</sup></xref><xref ref-type="aff" rid="AFF1"><sup>2</sup></xref><xref ref-type="aff" rid="AFF1"><sup>3</sup></xref></contrib></contrib-group><aff id="AFF1"><sup>1</sup>Institut Curie, Centre de Recherche, <sup>2</sup>INSERM, U900, Paris, F-75248 France and <sup>3</sup>Center for Computational Biology, Ecole des Mines de Paris, 35 rue saint Honore, 77305 Fontainebleau, France</aff><author-notes><corresp id="COR1">*To whom correspondence should be addressed.</corresp></author-notes><pub-date pub-type="ppub"><day>1</day><month>7</month><year>2008</year></pub-date><pub-date pub-type="pmc-release"><day>1</day><month>7</month><year>2008</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. --><volume>24</volume><issue>13</issue><fpage>i375</fpage><lpage>i382</lpage><permissions><copyright-statement>&#x000a9; 2008 The Author(s)</copyright-statement><copyright-year>2008</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc/2.0/uk/"><license-p><!--CREATIVE COMMONS-->This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/2.0/uk/">http://creativecommons.org/licenses/by-nc/2.0/uk/</ext-link>) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p><bold>Motivation:</bold> Array-based comparative genomic hybridization (arrayCGH) has recently become a popular tool to identify DNA copy number variations along the genome. These profiles are starting to be used as markers to improve prognosis or diagnosis of cancer, which implies that methods for automated supervised classification of arrayCGH data are needed. Like gene expression profiles, arrayCGH profiles are characterized by a large number of variables usually measured on a limited number of samples. However, arrayCGH profiles have a particular structure of correlations between variables, due to the spatial organization of bacterial artificial chromosomes along the genome. This suggests that classical classification methods, often based on the selection of a small number of discriminative features, may not be the most accurate methods and may not produce easily interpretable prediction rules.</p><p><bold>Results:</bold> We propose a new method for supervised classification of arrayCGH data. The method is a variant of support vector machine that incorporates the biological specificities of DNA copy number variations along the genome as prior knowledge. The resulting classifier is a sparse linear classifier based on a limited number of regions automatically selected on the chromosomes, leading to easy interpretation and identification of discriminative regions of the genome. We test this method on three classification problems for bladder and uveal cancer, involving both diagnosis and prognosis. We demonstrate that the introduction of the new prior on the classifier leads not only to more accurate predictions, but also to the identification of known and new regions of interest in the genome.</p><p><bold>Availability:</bold> All data and algorithms are publicly available.</p><p><bold>Contact:</bold> <email>franck.rapaport@curie.fr</email></p></abstract></article-meta></front><body><sec sec-type="intro" id="SEC1"><title>1 INTRODUCTION</title><p>Genome integrity is essential to cell life and is ensured in normal cells by a series of checkpoints, which enable DNA repair or trigger cell death to avoid abnormal genome cells to appear. The p53 protein is probably the most prominent protein known to play this role. When these checkpoints are bypassed the genome may evolve and undergo alterations to a point where the cell can become premalignant and further genome alterations lead to invasive cancers.</p><p>This genome instability has been shown to be an enabling characteristic of cancer (Hanahan and Weinberg, <xref ref-type="bibr" rid="B12">2000</xref>) and almost all cancers are associated with genome alterations. These alterations may be single mutations, translocations or copy number variations (CNVs). A CNV can be a deletion or a gain of small or large DNA regions, an amplification or an aneuploidy (change in chromosome number).</p><p>Many cancers present recurrent CNVs of the genome, like, for example, monoploidy of chromosome 3 in uveal melanoma (Speicher <italic>et al.</italic>, <xref ref-type="bibr" rid="B23">1994</xref>), loss of chromosome 9 and amplification of the region of cyclin D1 (11q13) in bladder carcinomas (Blaveri <italic>et al.</italic>, <xref ref-type="bibr" rid="B1">2005</xref>), loss of 1p and gain of 17q in neuroblastoma (Bown <italic>et al.</italic>, <xref ref-type="bibr" rid="B3">2001</xref>; Van Roy <italic>et al.</italic>, <xref ref-type="bibr" rid="B33">2002</xref>), EGFR amplification and deletion in 1p and 19q in gliomas (Idbaih <italic>et al.</italic>, <xref ref-type="bibr" rid="B13">2007</xref>) or amplifications of 1q, 8q24, 11q13, 17q21&#x02013;q23 and 20q13 in breast cancer (Yao <italic>et al.</italic>, <xref ref-type="bibr" rid="B36">2006</xref>). Moreover associations of specific alterations with clinical outcome have been described in many pathologies (Lastowska <italic>et al.</italic>, <xref ref-type="bibr" rid="B18">1997</xref>).</p><p>Recently array-based comparative genomic hybridization (arrayCGH) has been developed as a technique allowing rapid mapping of CNVs of a tumor sample at a genomic scale (Pinkel <italic>et al.</italic>, <xref ref-type="bibr" rid="B21">1998</xref>). The technique was first based on arrays using a few thousands of large insert clones (like bacterial artificial chromosomes, and with a mega base pair range resolution) to interrogate the genome, and then improved with oligonucleotide-based arrays consisting of several hundreds of thousands features, taking the resolution down to a few kilo base pairs (Gershon, <xref ref-type="bibr" rid="B11">2005</xref>). Many projects have since been launched to systematically detect genomic aberrations in cancer cells (Chin <italic>et al.</italic>, <xref ref-type="bibr" rid="B6">2006</xref>; Shing <italic>et al.</italic>, <xref ref-type="bibr" rid="B22">2003</xref>; van Beers and Nederlof, <xref ref-type="bibr" rid="B32">2006</xref>).</p><p>The etiology of cancer and the advent of arrayCGH make it natural to envisage building classifiers for prognosis or diagnosis based on the genomic profiles of tumors. Building classifiers based on expression profiles is an active field of research, but little attention has been paid yet to genome-based classification. Chin <italic>et al.</italic> (<xref ref-type="bibr" rid="B6">2006</xref>) select a small subset of genes and apply a <italic>k</italic>-nearest neighbor classifier to discriminate between estrogen-positive and estrogen-negative patients, between high-grade patients and low-grade patients and between bad prognosis and good prognosis for breast cancer. Jones <italic>et al.</italic> (<xref ref-type="bibr" rid="B14">2004</xref>) reduce the DNA copy number estimates to &#x02018;gains&#x02019; and &#x02018;losses&#x02019; at the chromosomal arm resolution, before using a nearest centroid method for classifying breast tumors according to their grade. As underlined in Chin <italic>et al.</italic> (<xref ref-type="bibr" rid="B6">2006</xref>), the classification accuracy reported in Jones <italic>et al.</italic> (<xref ref-type="bibr" rid="B14">2004</xref>) is better than the one reported in Chin <italic>et al.</italic> (<xref ref-type="bibr" rid="B6">2006</xref>), but still remains at a fairly high level with as much as 24% of misclassified samples in the balanced problem. This may be related to the higher resolution of the arrays produced by Jones <italic>et al.</italic> (<xref ref-type="bibr" rid="B14">2004</xref>). Moreover, the approach used by Jones <italic>et al.</italic> (<xref ref-type="bibr" rid="B14">2004</xref>) produces a classifier difficult to interpret as it is unable to detect any deletion or amplification that occur at the local level. O'Hagan <italic>et al.</italic> (<xref ref-type="bibr" rid="B19">2003</xref>) used a support vector machine (SVM) classifier using as variables all BAC ratios without any missing values. They were able to identify key CNAs.</p><p>The methods developed so far either ignore the particularities of arrayCGH and the inherent correlation structure of the data (O'Hagan <italic>et al.</italic>, <xref ref-type="bibr" rid="B19">2003</xref>), or drastically reduce the complexity of the data at the risk of filtering out useful information (Chin <italic>et al.</italic>, <xref ref-type="bibr" rid="B6">2006</xref>; Jones <italic>et al.</italic>, <xref ref-type="bibr" rid="B14">2004</xref>). In all cases, a reduction of the complexity of the data or a control of the complexity of the predictor estimated is needed to overcome the risk of overfitting the training data, given that the number of probes that form the profile is often several orders of magnitude larger than the number of samples available to train the classifier.</p><p>In this article, we propose a new method for supervised classification, specifically designed for the processing of arrayCGH profiles. In order not to miss potentially relevant information that may be lost if the profiles are first processed and reduced to a small number of homogeneous regions, we estimate directly a linear classifier at the level of individual probes. Yet, in order to control the risk of overfitting, we define a prior on the linear classifier to be estimated. This prior encodes the hypothesis that (i) many regions of the genome should not contribute to the classification rule (sparsity of the classifier) and (ii) probes that contribute to the classifier should be grouped in regions on the chromosomes, and be given the same weight within a region. This <italic>a priori</italic> information helps reducing the search space and produces a classification rule that is easier to interpret. This technique can be seen as an extension of SVM where the complexity of the classifier is controlled by a penalty function similar to the one used in the fused lasso method to enforce sparsity and similarity between successive features (Tibshirani <italic>et al.</italic>, <xref ref-type="bibr" rid="B29">2005</xref>). We, therefore, call the method a <italic>fused SVM</italic>. It produces a linear classifier that is piecewise constant on the chromosomes, and only involves a small number of loci without any a priori regularisation of the data. From a biological point of view, it avoids the prior choice of recurrent regions of alterations, but produces <italic>a posteriori</italic> a selection of discriminant regions, which are then amenable to further investigations.</p><p>We test the fused SVM on several public datasets involving diagnosis and prognosis applications in bladder and uveal cancer, and compare it with a more classical method involving feature selection without prior information about the organization of probes on the genome. In a cross-validation setting, we show that the classification rules obtained with the fused SVM are systematically more accurate than the rules obtained with the classical method, and that they are also more easily interpretable.</p></sec><sec sec-type="methods" id="SEC2"><title>2 METHODS</title><p>In this section, we present an algorithm for the supervised classification of arrayCGH data. This algorithm, which we call <italic>fused SVM</italic>, is motivated by the linear ordering of the features along the genome and the high dependency in behavior of neighboring features. The algorithm itself estimates a linear predictor by borrowing ideas from recent methods in regression, in particular, the <italic>fused lasso</italic> (Tibshirani <italic>et al.</italic>, <xref ref-type="bibr" rid="B29">2005</xref>). We start by a rapid description of the arrayCGH technology and data, before presenting the fused SVM in the context of regularized linear classification algorithms.</p><sec id="SEC2.1"><title>2.1 ArrayCGH data</title><p>ArrayCGH is a microarray-based technology that allows the quantification of the DNA copy number of a sample at many positions along the genome in a single experiment. The array contains thousands to millions of spots, each of them consisting of the amplified or synthesized DNA of a particular region of the genome. The array is hybridized with the DNA extracted from a sample of interest, and in most cases with (healthy) reference DNA. Both samples have first been labelled with two different fluorochromes, and the ratio of fluorescence of both fluorochromes is expected to reveal the ratio of DNA copy number at each position of the genome. The log-ratio profiles can then be used to detect the regions with abnormalities (log ratio significantly different of 0), corresponding to gains (if the log ratio is significantly superior to 0) or losses (if it is significantly inferior to 0).</p><p>The typical density of arrayCGH ranges from 2400 BAC features in the pioneering efforts, corresponding to one &#x0223c;100 kb probe every mega base pair (Pinkel <italic>et al.</italic>, <xref ref-type="bibr" rid="B21">1998</xref>), up to millions today, corresponding to one 25&#x02013;70 bp oligonucleotide probe every few kilo base pairs, or even tiling arrays (Gershon, <xref ref-type="bibr" rid="B11">2005</xref>).</p><p>There are two principal ways to represent arrayCGH data: as a log-ratio collection, or as a collection of status (lost, normal or gained, usually represented as &#x02212;1, 0 and 1, which correspond to the sign of the log ratio). The status representation has strong advantages over the log ratio as it reduces the complexity of the data, provides the scientist with a direct identification of abormalities and allows the straightforwad detection of recurrent alterations. However, converting ratios into status is not always obvious and often implies a loss of information which can be detrimental to the study: for several reasons such as heterogeneity of the sample or contamination with healthy tissue (which both result in cells with different copy numbers in the sample), the status may be difficult to infer from the data, whereas the use of the ratio values avoids this problem. Another problem is the low subtelty of statuses. In particular, if we want to use arrayCGH for discriminating between two subtypes of tumors or between tumors with different future evolution, all tumors may share the same important genomic alterations that are easily captured by the status assignment while differences between the types of tumors may be characterized by more subtle signals that would disappear should we transform the log-ratio values into statuses. Therefore, we consider below an arrayCGH profile as a vector of log ratios for all probes in the array.</p></sec><sec id="SEC2.2"><title>2.2 Classification of arrayCGH data</title><p>While much effort has been devoted to the analysis of single arrayCGH profiles, or populations of arrayCGH profiles in order to detect genomic alterations shared by the samples in the population, we focus on the supervised classification of arrayCGH. The typical problem we want to solve is, given two populations of arrayCGH data corresponding to two populations of samples, to design a classifier that is able to predict which population any new sample belongs to. This paradigm can be applied for diagnosis or prognosis applications, where the populations are respectively samples of different tumor types, or with different evolution. Although we only focus here on binary classification, the techniques can be easily extended to problems involving more than two classes using, for example, a series of binary classifiers trained to discriminate each class against all others.</p><p>While accuracy is certainly the first quality we want the classifier to have in real diagnosis and prognosis application, it is also important to be able to interpret it and understand what the classification is based on. Therefore, we focus on linear classifiers, which associate a weight to each probe and produce a rule that is based on a linear combination of the probe log ratios. The weight of a probe roughly corresponds to its contribution in the final classification rule, and therefore provides evidence about its importance as a marker to discriminate the populations. It should be pointed out, however, that when correlated features are present, the weight of a feature is not directly related to the individual correlation of the feature with the classification, hence some care should be taken for the interpretation of linear classifier.</p><p>In most applications of arrayCGH classification, it can be expected that only a limited number of regions on the genome should contribute to the classification, because most parts of the genome may not differ between populations. Moreover, the notion of discriminative regions suggest that a good classifier should detect these regions, and typically be piecewise constant over them. We show below how to introduce these prior hypotheses into the linear classification algorithm.</p></sec><sec id="SEC2.3"><title>2.3 Linear supervised classification</title><p>Let us denote by <italic>p</italic> the number of probes hybridized on the arrayCGH. The result of an arrayCGH competitive hybridization is then a vector of <italic>p</italic> log ratios, which we represent by a vector <italic>x</italic> in the vector space &#x1d4b3;=&#x0211d;<sup><italic>p</italic></sup> of possible arrayCGH profiles. We assume that the samples to be hybridized can belong to two classes, which we represent by the labels &#x02212;1 and +1. The classes typically correspond to the disease status or the prognosis of the samples. The aim of binary classification is to find a decision function that can predict the class <italic>y</italic>&#x02208;{&#x02212;1,+1} of a data sample <italic>x</italic>&#x02208;&#x1d4b3;. Supervised classification uses a database of samples <italic>x</italic><sub>1</sub>,&#x02026;, <italic>x</italic><sub><italic>n</italic></sub> &#x02208;&#x1d4b3; for which the labels <italic>y</italic><sub>1</sub>,&#x02026;, <italic>y</italic><sub><italic>n</italic></sub> &#x02208;{&#x02212;1,+1} are known in order to construct the prediction function. We focus on <italic>linear</italic> decision functions, which are defined by functions of the form <italic>f</italic>(<italic>x</italic>)=<italic>w</italic><sup>&#x022a4;</sup><italic>x</italic> where <italic>w</italic><sup>&#x022a4;</sup> is the transpose of a vector <italic>w</italic>&#x02208;&#x0211d;<sup><italic>d</italic></sup>. The class prediction for a profile <italic>x</italic> is then +1 if <italic>f</italic>(<italic>x</italic>)&#x02265;0, and &#x02212;1 otherwise. Training a linear classifier amounts to estimating a vector <italic>w</italic>&#x02208;&#x0211d;<sup><italic>d</italic></sup> from prior knowledge and the observation of the labeled training set.</p><p>The training set can be used to assess whether a candidate vector <italic>w</italic> can correctly predict the labels on the training set; one may expect such a <italic>w</italic> to correctly predict the classes of unlabeled samples as well. This induction principle, sometimes referred to as <italic>empirical risk minimization</italic> is, however, likely to fail in our situation where the dimension of the samples (the number of probes) is typically larger than the number of training points. In such a case, many vectors <italic>w</italic> can indeed perfectly explain the labels of the training set, without capturing any biological information. These vectors are likely to poorly predict the classes of new samples. A well-known strategy to overcome this overfitting issue, in particular when the dimension of the data is large compared to the number of training points available, is to look for <italic>large-margin</italic> classifiers constrained by <italic>regularization</italic> (Vapnik , <xref ref-type="bibr" rid="B34">1998</xref>). A large-margin classifier is a prediction function <italic>f</italic>(<italic>x</italic>) that not only tends to produce the correct sign (positive for labels +1, negative for class &#x02212;1), but also tends to produce large absolute values. This can be formalized by the notion of <italic>margin</italic>, defined as <italic>yf</italic>(<italic>x</italic>): large-margin classifiers try to predict the class of a sample with large margin. Note that the prediction is correct if the margin is positive. The margin can be thought of as a measure of confidence in the prediction given by the sign of <italic>f</italic>, so a large margin is synonymous with a large confidence. Training a large-margin classifier means estimating a function <italic>f</italic> that takes large-margin values on the training set. However, just like for the sign of <italic>f</italic>, if <italic>p</italic>&#x0003e;<italic>n</italic> then it is possible to find vectors <italic>w</italic> that lead to arbitrarily large margin on all points of the training set. In order to control this overfitting, large-margin classifiers try to maximize the margin of the classifier on the training set under some additional constraint on the classifier <italic>f</italic>, typically that <italic>w</italic> is not too &#x02018;large&#x02019;. In summary, large-margin classifiers find a trade-off between the objective to ensure large-margin values on the training set, on the one hand, and that of controlling the complexity of the classifier, on the other hand. The balance in this trade-off is typically controlled by a parameter of the algorithm.</p><p>More formally, large-margin classifiers typically require the definition of two ingredients:
<list list-type="bullet"><list-item><p>A <italic>loss function l</italic>(<italic>t</italic>) that is &#x02018;small&#x02019; when <italic>t</italic>&#x02208;&#x0211d; is &#x02018;large&#x02019;. From the loss function one can deduce the <italic>empirical risk</italic> of a candidate vector <italic>w</italic>, given by the average loss function applied to the margins of <italic>w</italic> on the training set:
<disp-formula id="M1"><label>(1)</label><graphic xlink:href="btn188m1"/></disp-formula>
The smaller the empirical risk, the better <italic>w</italic> fits the training set in the sense of having a large margin. Typical loss functions are the hinge loss <italic>l</italic>(<italic>t</italic>)=max(0,1&#x02212;<italic>t</italic>) and the logit loss <italic>l</italic>(<italic>t</italic>)=log(1+<italic>e</italic><sup>&#x02212;<italic>t</italic></sup>).</p></list-item><list-item><p>A <italic>penalty function</italic> &#x003a9;(<italic>w</italic>) that measures how &#x02018;large&#x02019; or how &#x02018;complex&#x02019; <italic>w</italic> is. Typical penalty functions are the <italic>L</italic><sub>1</sub> and <italic>L</italic><sub>2</sub> norms of <italic>w</italic>, defined respectively by <inline-formula><inline-graphic xlink:href="btn188i1.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="btn188i2.jpg"/></inline-formula>.</p></list-item></list>
Given a loss function <italic>l</italic> and a penalty function &#x003a9;, large-margin classifiers can then be trained on a given training set by solving the following constrained optimization problem:
<disp-formula id="M2"><label>(2)</label><graphic xlink:href="btn188m2"/></disp-formula>
where &#x003bc; is a parameter that controls the trade-off between fitting the data, i.e. minimizing <italic>R</italic><sub><italic>emp</italic></sub>(<italic>f</italic>), and monitoring the regularity of the classifier, i.e. monitoring &#x003a9;(<italic>w</italic>). Examples of large-margin classifiers include the SVM and kernel logistic regression (KLR) obtained by combining respectively the hinge and logit losses with the <italic>L</italic><sub>2</sub> norm penalization function (Boser <italic>et al.</italic>, <xref ref-type="bibr" rid="B2">1992</xref>; Cortes and Vapnik , <xref ref-type="bibr" rid="B8">1995</xref>; Vapnik , <xref ref-type="bibr" rid="B34">1998</xref>) or the 1-norm SVM when the hinge loss is combined with the <italic>L</italic><sub>1</sub> loss.</p><p>The final classifier depends on both the loss function and the penalty function. In particular, the penalty function is useful to include prior knowledge or intuition about the classifier one expects. For example, the <italic>L</italic><sub>1</sub> penalty function is widely used because it tends to produce sparse vectors <italic>w</italic>, therefore, performing an automatic selection of features. This property has been successfully used in the context of regression (Tibshirani , <xref ref-type="bibr" rid="B26">1996</xref>), signal representation (Chen <italic>et al.</italic>, <xref ref-type="bibr" rid="B5">1998</xref>), survival analysis (Tibshirani , <xref ref-type="bibr" rid="B27">1997</xref>), logistic regression (Genkin <italic>et al.</italic>, <xref ref-type="bibr" rid="B10">2007</xref>; Krishnapuram <italic>et al.</italic>, <xref ref-type="bibr" rid="B15">2004</xref>) or multinomial logistic regression (Krishnapuram <italic>et al.</italic>, <xref ref-type="bibr" rid="B16">2005</xref>), where one expects to estimate a sparse vector.</p></sec><sec id="SEC2.4"><title>2.4 Fused lasso</title><p>Some authors have proposed to design-specific penalty functions as a means to encode specific prior informations about the expected form of the final classifier. In the context of regression applied to signal processing, when the data is a time series, Land and Friedman, (<xref ref-type="bibr" rid="B17">1996</xref>) propose to encode the expected positive correlation between successive variables by choosing a regularization term that forces successive variables of the classifier to have similar weights. More precisely, assuming that the variables <italic>w</italic><sub>1</sub>,<italic>w</italic><sub>2</sub>,&#x02026;,<italic>w</italic><sub><italic>p</italic></sub> are sorted in a natural order where many pairs of successive values are expected to have the same weight, they propose the <italic>variable fusion</italic> penalty function:
<disp-formula id="M3"><label>(3)</label><graphic xlink:href="btn188m3"/></disp-formula>
Plugging this penalty function in the general algorithm (<xref ref-type="disp-formula" rid="M2">2</xref>) enforces a solution <italic>w</italic> with many successive values equal to each others, that is, tends to produce a piecewise constant weight vector. In order to combine this interesting property with a requirement of sparseness of the solution, (Tibshirani <italic>et al.</italic>, <xref ref-type="bibr" rid="B29">2005</xref>) proposed to combine the lasso penalty and the variable fusion penalty into a single optimization problem with two constraints, namely:
<disp-formula id="M4"><label>(4)</label><graphic xlink:href="btn188m4"/></disp-formula>
where &#x003bb; and &#x003bc; are two parameters that control the relative trade-offs between fitting the training data (small <italic>R</italic><sub>emp</sub>), enforcing sparsity of the solution (small &#x003bb;) and enforcing the solution to be piecewise constant (small &#x003bc;). When the empirical loss is the mean square error in regression, the resulting algorithm is called <italic>fused lasso</italic>. This method was illustrated in (Tibshirani <italic>et al.</italic>, <xref ref-type="bibr" rid="B29">2005</xref>) with examples taken from gene expression datasets and mass spectrometry. Later, Tibshirani and Wang (<xref ref-type="bibr" rid="B28">2007</xref>) proposed a tweak of the fused lasso for the purpose of signal smoothing, and illustrated it for the problem of discretizing noisy CGH profiles.</p></sec><sec id="SEC2.5"><title>2.5 Fused SVM</title><p>Remembering from <xref ref-type="sec" rid="SEC2.2">Section 2.2</xref> that for arrayCGH data classification one typically expects the &#x02018;true&#x02019; classifier to be sparse and piecewise constant along the genome, we propose to extend the fused lasso to the context of classification and adapt it to the chromosome structure for arrayCGH data classification. The extension of fused lasso from regression to large-margin classification is obtained simply by plugging the fused lasso penalty constraints into a large-margin empirical risk in (<xref ref-type="disp-formula" rid="M4">4</xref>). In what follows we focus on the empirical risk (<xref ref-type="disp-formula" rid="M1">1</xref>) obtained from the hinge loss, which leads to a simple implementation as a linear program (see <xref ref-type="sec" rid="SEC2.6">Section 2.6</xref> subsequently). The extension to other convex loss functions, in particular the logit loss function, results in convex optimization problems with linear constraints that can be solved with general convex optimization solvers (Boyd and Vandenberghe , <xref ref-type="bibr" rid="B4">2004</xref>).</p><p>In the case of arrayCGH data, a minor modification to the variable fusion penalty (<xref ref-type="disp-formula" rid="M3">3</xref>) is necessary to take into account the structure of the genome in chromosomes. Indeed, two successive spots on the same chromosome are prone to be subject to the same amplification and are therefore likely to have similar weights on the classifier; however, this positive correlation is not expected across different chromosomes. Therefore, we restrict the pairs of successive features appearing in the function constraint (<xref ref-type="disp-formula" rid="M3">3</xref>) to be consecutive probes on the same chromosome.</p><p>We call the resulting algorithm a <italic>fused SVM</italic>, which can be formally written as the solution of the following problem:
<disp-formula id="M5"><label>(5)</label><graphic xlink:href="btn188m5"/></disp-formula>
where <italic>i</italic>&#x0223c;<italic>j</italic> if <italic>i</italic> and <italic>j</italic> are the indices of succesive spots of the same chromosome. As with fused lasso, this optimization problem tends to produce classifiers <italic>w</italic> with similar weights for consecutive features, while maintaining its sparseness. This algorithm depends on two paramters, &#x003bb; and &#x003bc;, which are typically chosen via cross-validation on the training set. Decreasing &#x003bb; tends to increase the sparsity of <italic>w</italic>, while decreasing &#x003bc; tends to enforce successive spots to have the same weight.</p><p>This classification algorithm can be applied to CGH profiles, taking the ratios as features. Due to the effect of both regularization terms, we obtain a sparse classification function that attributes similar weights to successive spots.</p></sec><sec id="SEC2.6"><title>2.6 Implementation of the fused SVM</title><p>Introducing slack variables, the problem described in (<xref ref-type="disp-formula" rid="M5">5</xref>) is equivalent to the following linear program:
<disp-formula id="M6"><label>(6)</label><graphic xlink:href="btn188m6"/></disp-formula>
In our experiments, we implemented and solved this problem using Matlab and the SeDuMi 1.1R3 optimization toolbox (Sturm, <xref ref-type="bibr" rid="B25">1999</xref>).</p></sec></sec><sec id="SEC3"><title>3 DATA</title><p>We consider two publicly available arrayCGH datasets for cancer research, from which we deduce three problems of diagnosis and prognosis to test our method.</p><p>The first dataset contains arrayCGH profiles of 57 bladder tumor samples (Stransky <italic>et al.</italic>, <xref ref-type="bibr" rid="B24">2006</xref>). Each profile gives the relative quantity of DNA for 2215 spots. We removed the probes corresponding to sexual chromosomes, because the sex mismatch between some patients and the reference used makes the computation of copy number less reliable, giving us a final list of 2143 spots. We considered two types of tumor classification: either by grade, with 12 tumors of Grade 1 and 45 tumors of higher grades (2 or 3) or by stage, with 16 tumors of Stage Ta and 32 tumors of Stage T2+. In the case of stage classification, 9 tumors with intermediary Stage T1 were excluded from the classification.</p><p>The second dataset contains arrayCGH profiles for 78 melanoma tumors that have been arrayed on 3750 spots (Trolet <italic>et al.</italic>, <xref ref-type="bibr" rid="B30">2008</xref>). As for the bladder cancer dataset, we excluded the sexual chromosomes from the analysis, resulting in a total of 3649 spots. 35 of these tumors lead to the development of liver metastases within 24 months, while 43 did not. We, therefore, consider the problem of predicting, from an arrayCGH profile, whether or not the tumor will metastasize within 24 months.</p><p>In both datasets, we replaced the missing spots log ratios by 0. In order to assess the performance of a classification method, we performed a cross-validation for each of the three classification problems, following a leave-one-out (LOO) procedure for the bladder dataset and a 10-fold procedure for the melanoma dataset. We measure the number of misclassified samples for different values of parameters &#x003bb; and &#x003bc;.</p></sec><sec sec-type="results" id="SEC4"><title>4 RESULTS</title><p>In this section, we present the results obtained with the fused SVM on the datasets described in the previous section. As a baseline method, we consider a <italic>L</italic><sub>1</sub>-SVM which minimizes the mean empirical hinge loss subject to a constraint on the <italic>L</italic><sub>1</sub> norm of the classifier in (<xref ref-type="disp-formula" rid="M2">2</xref>). The <italic>L</italic><sub>1</sub>-SVM performs automatic feature selection, and a regularization parameter &#x003bb; controls the amount of regularization. It has been shown to be a competitive classification method for high-dimensional data, such as gene-expression data (Zhu <italic>et al.</italic>, <xref ref-type="bibr" rid="B37">2004</xref>). In fact, the <italic>L</italic><sub>1</sub>-SVM is a particular case of our fused SVM, when the &#x003bc; parameter is chosen large enough to relax the variable fusion constraint (<xref ref-type="disp-formula" rid="M3">3</xref>), typically by taking &#x003bc;&#x0003e;2&#x003bb;. Hence, by varying &#x003bc; from a large value to 0, we can see the effect of the variable fusion penalty on the classical <italic>L</italic><sub>1</sub>-SVM.</p><sec id="SEC4.1"><title>4.1 Bladder tumors</title><p>The upper plot of <xref ref-type="fig" rid="F1">Figure 1</xref> show the estimated accuracy (by LOO) of the fused SVM as a function of the regularization parameters &#x003bb; and &#x003bc;, for the classification by grade of the bladder tumors. The middle plot of <xref ref-type="fig" rid="F1">Figure 1</xref> represents the best linear classifier found by the <italic>L</italic><sub>1</sub>-SVM (corresponding to &#x003bb;=256), while the lower plot shows the linear classifier estimated from all samples by the fused SVM when &#x003bb; and &#x003bc; are set to values that minimize the LOO error, namely &#x003bb;=32 and &#x003bc;=1. Similarly, <xref ref-type="fig" rid="F2">Figure 2</xref> shows the same results (LOO accuracy, <italic>L</italic><sub>1</sub>-SVM and fused SVM classifiers) for the classification of bladder tumors according to their stage.
<fig id="F1" position="float"><label>Fig. 1.</label><caption><p>The figure on the upper side represents the number of misclassified samples in a LOO error loop on the bladder cancer dataset with the grade labeling, with its color scale for different values of the parameters &#x003bb; and &#x003bc; which vary logarithmically along the axes. The weights of the best classifier, for classical <italic>L</italic><sub>1</sub>-SVM (middle) and for fused SVM (lower part) are ordered and represented in a blue line, annotated with the chromosome separation (red line).</p></caption><graphic xlink:href="btn188f1"/></fig>
<fig id="F2" position="float"><label>Fig. 2.</label><caption><p>The figure on the upper side represents the number of misclassified samples in a LOO error loop on the bladder cancer dataset with the stage labeling, with its color scale, for different values of the parameters &#x003bb; and &#x003bc; which vary logarithmically along the axes. The weights of the best classifier, for classical <italic>L</italic><sub>1</sub>-SVM (middle) and for fused-SVM (lower part) are ordered and represented in a blue line, annotated with the chromosome separation (red line).</p></caption><graphic xlink:href="btn188f2"/></fig></p><p>In both cases, when &#x003bc; is large enough to make the variable fusion inactive in (<xref ref-type="disp-formula" rid="M5">5</xref>), then the classifier only finds a compromise between the empirical risk and the <italic>L</italic><sub>1</sub> norm of the classifier. In other words, we recover the classical <italic>L</italic><sub>1</sub> SVM with parameter &#x003bb;. Graphically, the performance of the <italic>L</italic><sub>1</sub> SVM for varying &#x003bb; can be seen on the upper side of each plot of the LOO accuracy in <xref ref-type="fig" rid="F1">Figures 1</xref> and <xref ref-type="fig" rid="F2">2</xref>. Interestingly, in both cases we observe that the best performance obtained when both &#x003bb; and &#x003bc; can be adjusted is much better than the best performance of the <italic>L</italic><sub>1</sub>-SVM, when only &#x003bb; can be adjusted. In the case of grade classification, the number of misclassified samples drops from 12 (21%) to 7 (12%), while in the case of stage classification it drops from 13 (28%) to 7 (15%). This suggests that the additional constraint that translates our prior knowlege about the structure of the spot positions on the genome is beneficial in terms of classifier accuracy.</p><p>As expected, there are also important differences in the visual aspects of the classifiers estimated by the <italic>L</italic><sub>1</sub>-SVM and the fused SVM. The fused SVM produces sparse and piecewise constant classifiers, amenable to further investigations, while it is more difficult to isolate from the <italic>L</italic><sub>1</sub>-SVM profiles the key features used in the classification, apart from a few strong peaks.</p><p>As we can see by looking at the shape of the fused SVM classifier in <xref ref-type="fig" rid="F1">Figure 1</xref>, the grade classification function is characterized by non-null constant values over a few small chromosomal regions and numerous larger regions. Of these regions, a few are already known as being altered in bladder tumors, such as the gain on region 1q (Corson <italic>et al.</italic>, <xref ref-type="bibr" rid="B7">2005</xref>). Moreover some of them have already been shown to be correlated with grade, such as chromosome 7 (Waldman <italic>et al.</italic>, <xref ref-type="bibr" rid="B35">1991</xref>).</p><p>On the contrary, the stage classifier is characterised by only a few regions with most of them involving large portions of chromosomes. They concern mainly chromosome 4, 7, 8q, 11p, 14, 15, 17, 20, 21 and 22, with in particular a strong contribution from chromosomes 4, 7 and 20. These results on chromosomes 7, 8q, 11p and 20 are in good agreement with Blaveri <italic>et al.</italic> (<xref ref-type="bibr" rid="B1">2005</xref>) who identified the most common alterations according to tumor stage on a set of 98 bladder tumors.</p></sec><sec id="SEC4.2"><title>4.2 Melanoma tumors</title><p>Similar to <xref ref-type="fig" rid="F1">Figures 1</xref> and <xref ref-type="fig" rid="F2">2</xref>, the three plots in <xref ref-type="fig" rid="F3">Figure 3</xref> show respectively the accuracy, estimated by 10-fold cross-validation, of the fused SVM as a function of the regularization parameters &#x003bb; and &#x003bc;, the linear classifier estimated by the <italic>L</italic><sub>1</sub>-SVM when &#x003bb; is set to the value that minimizes the estimated error (&#x003bb;=4), and the linear classifier estimated by a fused SVM on all samples when &#x003bb; and &#x003bc; are set to values that minimize the 10-fold error, namely &#x003bb;=64 and &#x003bc;=0.5.
<fig id="F3" position="float"><label>Fig. 3.</label><caption><p>The figure on the upper part represents the number of misclassified samples in a 10-fold error loop on the melanoma dataset. The weights of the best classifier, for classical <italic>L</italic><sub>1</sub>-SVM (middle) and for fused SVM (lower part) are ordered and represented in a blue line, annotated with the chromosome separation (red line).</p></caption><graphic xlink:href="btn188f3"/></fig></p><p>Similar to the bladder study, the performance of the <italic>L</italic><sub>1</sub>-SVM without the fusion constraint can be retrieved by looking at the upper part of the plot of <xref ref-type="fig" rid="F3">Figure 3</xref>. The fused classifier offers a slightly improved performance compared to the standard <italic>L</italic><sub>1</sub>-SVM [17 errors (22%) versus 19 errors (24%)], even though the amelioration seems more marginal compared to the improvement made with bladder tumors and the misclassification rate remains fairly high.</p><p>As for the bladder datasets, the <italic>L</italic><sub>1</sub>-SVM and fused SVM classifiers are markedly different. The <italic>L</italic><sub>1</sub>-SVM classifier is based only on a few BAC concentrated on chromosome 8, with positive weights on the 8p arm and negative weights on the 8q arm. These features are biologically relevant and correspond to a known genomic alterations (loss of 8p and gain of 8q in metastatic tumors). The presence of a strong signal concentrated on chromosome 8 for the prediction of metastasis is in this case correctly captured by the sparse <italic>L</italic><sub>1</sub>-SVM, which explains its relatively good performance.</p><p>On the contrary, the fused SVM classifier is characterized by many CNAs, most of them involving large regions of chromosomes. Interestingly, we retrieve the regions whose alteration was already reported as recurrent events of uveal melanoma: chromosomes 3, 1p, 6q, 8p, 8q, 16q. As expected the contributions of 8p and 8q are of opposite sign, in agreement with the common alterations of these regions: loss of 8p and gain of 8q in etastatic tumors. Interestingly, the contribution of chromosome 3 is limited to a small region of 3p, and does not involve the whole chromosome as the frequency of chromosome 3 monosomy would have suggested. Note that this is consistent with works by Parrella <italic>et al.</italic> (<xref ref-type="bibr" rid="B20">2003</xref>) and Tschentscher <italic>et al.</italic> (<xref ref-type="bibr" rid="B31">2001</xref>) who delimited small 3p regions from partial chromosome 3 deletion patients. On the other hand, we also observe that large portions of other chromosomes have been assigned significant positive or negative weights, such as chromosomes 1p, 2p, 4, 5, 9q, 11p, 12q, 13, 14, 20, 21. To our knowledge, they do not correspond to previous observations, and may therefore provide interesting starting points for further investigations.</p></sec></sec><sec sec-type="discussion" id="SEC5"><title>5 DISCUSSION</title><p>We have proposed a new method for the supervised classification of arrayCGH data. Thanks to the use of a particular regularization term that translates our prior assumptions into constraints on the classifier, we estimate a linear classifier that is based on a restricted number of spots, and gives as much as possible equal weights to spots located near each other on a chromosome. Results on real datasets show that this classification method is able to discriminate between the different classes with a better performance than classical techniques that do not take into account the specificities of arrayCGH data. Moreover, the learned classifier is piecewise constant, and therefore lends itself particularly well to further interpretation, highlighting, in particular selected chromosomal regions with particularly highly positive or negative weights.</p><p>From the methodological point of view, the use of regularized large-scale classifiers is nowadays widely spread, especially in the SVM form. Regularization is particularly important for &#x02018;small <italic>n</italic> large <italic>p</italic>&#x02019; problems, i.e. when the number of samples is small compared to the number of dimensions. An alternative interpretation of such classifiers is that they correspond to maximum <italic>a posteriori</italic> classifiers in a Bayesian framework, where the prior over classifier is encoded in our penalty function. It is not surprising, then, that encoding prior knowledge in the penalty function is a mathematically sound strategy that can be strongly beneficial in terms of classifier accuracy, in particular when few training samples are available. The accuracy improvements we observe on all classification datasets confirm this intuition. Besides the particular penalty function investigated in this article, we believe our results support the general idea that engineering relevant priors for a particular problem can have important effects on the quality of the function estimated and paves the way for further research on the engineering of such priors in combination with large-margin classifiers. As for the implementation, we solved a linear program for each value couple of the regularization parameters &#x003bb; and &#x003bc;, but it would be interesting to generalize the recent works on path following algorithms to be able to follow the solution of the optimization problem when &#x003bb; and &#x003bc; vary (Efron <italic>et al.</italic>, <xref ref-type="bibr" rid="B9">2004</xref>).</p><p>Another interesting direction of future research concerns the combination of heterogeneous data, in particular of arrayCGH and gene expression data. Gene expression variations contain indeed information complementary to CNV for the genetic aberrations of the dysfunctioning cell (Stransky <italic>et al.</italic>, <xref ref-type="bibr" rid="B24">2006</xref>), and their combination is therefore likely to both improve the accuracy of the classification methods and shed new light on biological phenomena that are characteristic of each class. A possible strategy to combine such datasets would be to train a large-margin classifier with a particular regularization term that should be adequately designed.</p></sec></body><back><ack><title>ACKNOWLEDGEMENTS</title><p>We thank J&#x000e9;r&#x000f4;me Couturier, Sophie Piperno-Neumann and Simon Saule, and the uveal melanoma group from Institut Curie. We are also grateful to Philippe Hup&#x000e9; for his help in preparing the data.</p><p><italic>Funding:</italic> This project was partly funded by the ACI IMPBIO Kernelchip and the EC contract ESBIC-D (LSHG-CT-2005-518192). F.R. and E.B. are members of the team &#x02018;Systems Biology of Cancer&#x02019;, Equipe labellis&#x000e9;e par la Ligue Nationale Contre le Cancer.</p><p><italic>Conflict of Interest:</italic> none declared.</p></ack><ref-list><title>REFERENCES</title><ref id="B1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blaveri</surname><given-names>E</given-names></name><etal/></person-group><article-title>Bladder cancer stage and outcome by array-based comparative genomic hybridization</article-title><source>Clin. Cancer Res</source><year>2005</year><volume>11</volume><fpage>7012</fpage><lpage>7022</lpage><pub-id pub-id-type="pmid">16203795</pub-id></element-citation></ref><ref id="B2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Boser</surname><given-names>BE</given-names></name><etal/></person-group><article-title>A training algorithm for optimal margin classifiers</article-title><source>COLT'92: Proceedings of the fifth annual workshop on Computational learning theory</source><year>1992</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>ACM</publisher-name><fpage>144</fpage><lpage>152</lpage></element-citation></ref><ref id="B3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bown</surname><given-names>N</given-names></name><etal/></person-group><article-title>17q gain in neuroblastoma predicts adverse clinical outcome. U.K. cancer cytogenetics group and the U.K. children's cancer study group</article-title><source>Med. Pediatr. Oncol</source><year>2001</year><volume>36</volume><fpage>14</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">11464868</pub-id></element-citation></ref><ref id="B4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Boyd</surname><given-names>S</given-names></name><name><surname>Vandenberghe</surname><given-names>L</given-names></name></person-group><source>Convex Optimization</source><year>2004</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="B5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>SS</given-names></name><etal/></person-group><article-title>Atomic decomposition by basis pursuit</article-title><source>SIAM J. Sci. Comput</source><year>1998</year><volume>20</volume><fpage>33</fpage><lpage>61</lpage></element-citation></ref><ref id="B6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chin</surname><given-names>SF</given-names></name><etal/></person-group><article-title>Using array-comparative genomic hybridization to define molecular portraits of primary breast cancers</article-title><source>Oncogene</source><year>2006</year><volume>26</volume><fpage>1959</fpage><lpage>1970</lpage><pub-id pub-id-type="pmid">17001317</pub-id></element-citation></ref><ref id="B7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corson</surname><given-names>TW</given-names></name><etal/></person-group><article-title>Kif14 is a candidate oncogene in the 1q minimal region of genomic gain in multiple cancers</article-title><source>Oncogene</source><year>2005</year><volume>24</volume><fpage>4741</fpage><lpage>4753</lpage><pub-id pub-id-type="pmid">15897902</pub-id></element-citation></ref><ref id="B8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortes</surname><given-names>C</given-names></name><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><article-title>Support-vector networks</article-title><source>Machine Learning</source><year>1995</year><volume>20</volume><fpage>273</fpage><lpage>297</lpage></element-citation></ref><ref id="B9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Efron</surname><given-names>B</given-names></name><etal/></person-group><article-title>Least angle regression</article-title><source>Ann. Stat</source><year>2004</year><volume>32</volume><fpage>407</fpage><lpage>499</lpage></element-citation></ref><ref id="B10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genkin</surname><given-names>A</given-names></name><etal/></person-group><article-title>Large-scale Bayesian logistic regression for text categorization</article-title><source>Technometrics</source><year>2007</year><volume>49</volume><fpage>291</fpage><lpage>304</lpage></element-citation></ref><ref id="B11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershon</surname><given-names>D</given-names></name></person-group><article-title>DNA microarrays: more than gene expression</article-title><source>Nature</source><year>2005</year><volume>437</volume><fpage>1195</fpage><lpage>1198</lpage><pub-id pub-id-type="pmid">16237450</pub-id></element-citation></ref><ref id="B12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanahan</surname><given-names>D</given-names></name><name><surname>Weinberg</surname><given-names>RA</given-names></name></person-group><article-title>The hallmarks of cancer</article-title><source>Cell</source><year>2000</year><volume>100</volume><fpage>57</fpage><lpage>70</lpage><pub-id pub-id-type="pmid">10647931</pub-id></element-citation></ref><ref id="B13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Idbaih</surname><given-names>A</given-names></name><etal/></person-group><article-title>BAC array CGH distinguishes mutually exclusive alterations that define clinicogenetic subtypes of gliomas</article-title><source>Int. J. Cancer</source><year>2007</year></element-citation></ref><ref id="B14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>C</given-names></name><etal/></person-group><article-title>Molecular cytogenetic identification of subgroups of grade III invasive ductal breast carcinomas with different clinical outcomes</article-title><source>Clin. Cancer Res</source><year>2004</year><volume>10</volume><fpage>5988</fpage><lpage>5997</lpage><pub-id pub-id-type="pmid">15447982</pub-id></element-citation></ref><ref id="B15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnapuram</surname><given-names>B</given-names></name><etal/></person-group><article-title>A Bayesian approach to joint feature selection and classifier design</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell</source><year>2004</year><volume>26</volume><fpage>1105</fpage><lpage>11</lpage><pub-id pub-id-type="pmid">15742887</pub-id></element-citation></ref><ref id="B16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnapuram</surname><given-names>B</given-names></name><etal/></person-group><article-title>Sparse multinomial logistic regression: fast algorithms and generalization bounds</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2005</year><volume>27</volume><fpage>957</fpage><lpage>968</lpage><pub-id pub-id-type="pmid">15943426</pub-id></element-citation></ref><ref id="B17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Land</surname><given-names>SR</given-names></name><name><surname>Friedman</surname><given-names>JH</given-names></name></person-group><article-title>Variable fusion: a new adaptive signal regression method</article-title><source>Technical Report</source><year>1996</year></element-citation></ref><ref id="B18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lastowska</surname><given-names>M</given-names></name><etal/></person-group><article-title>Comparative genomic hybridization study of primary neuroblastoma tumors. united kingdom children's cancer study group</article-title><source>Genes Chromosomes Cancer</source><year>1997</year><volume>18</volume><fpage>162</fpage><lpage>169</lpage><pub-id pub-id-type="pmid">9071568</pub-id></element-citation></ref><ref id="B19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Hagan</surname><given-names>RC</given-names></name><etal/></person-group><article-title>Array comparative genome hybridization for tumor classification and gene discovery in mouse models of malignant melanoma</article-title><source>Cancer Res</source><year>2003</year><volume>63</volume><fpage>5352</fpage><lpage>5356</lpage><pub-id pub-id-type="pmid">14500367</pub-id></element-citation></ref><ref id="B20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parrella</surname><given-names>P</given-names></name><etal/></person-group><article-title>Fine mapping of chromosome 3 in uveal melanoma: identification of a minimal region of deletion on chromosomal arm 3p25.1-p25.2</article-title><source>Cancer Res</source><year>2003</year><volume>63</volume><fpage>8507</fpage><lpage>8510</lpage><pub-id pub-id-type="pmid">14679017</pub-id></element-citation></ref><ref id="B21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinkel</surname><given-names>D</given-names></name><etal/></person-group><article-title>High resolution analysis of DNA copy number variation using comparative genomic hybridization to microarrays</article-title><source>Nat. Genet</source><year>1998</year><volume>20</volume><fpage>207</fpage><lpage>211</lpage><pub-id pub-id-type="pmid">9771718</pub-id></element-citation></ref><ref id="B22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shing</surname><given-names>DC</given-names></name><etal/></person-group><article-title>FUS/ERG gene fusions in Ewing's tumors</article-title><source>Cancer Res</source><year>2003</year><volume>63</volume><fpage>4568</fpage><lpage>4576</lpage><pub-id pub-id-type="pmid">12907633</pub-id></element-citation></ref><ref id="B23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Speicher</surname><given-names>M</given-names></name><etal/></person-group><article-title>Chromosomal gains and losses in uveal melanomas detected by comparative genomic hybridization</article-title><source>Cancer Res</source><year>1994</year><volume>54</volume><fpage>3817</fpage><lpage>23</lpage><pub-id pub-id-type="pmid">8033101</pub-id></element-citation></ref><ref id="B24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stransky</surname><given-names>N</given-names></name><etal/></person-group><article-title>Regional copy number-independent deregulation of transcription in cancer</article-title><source>Nat. Genet</source><year>2006</year><volume>38</volume><fpage>1386</fpage><lpage>1396</lpage><pub-id pub-id-type="pmid">17099711</pub-id></element-citation></ref><ref id="B25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sturm</surname><given-names>J</given-names></name></person-group><article-title>Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones</article-title><source>Optimization Methods and Software</source><year>1999</year><volume>11&#x02013;12</volume><fpage>625</fpage><lpage>653</lpage><comment>Special issue on Interior Point Methods (CD supplement with software).</comment></element-citation></ref><ref id="B26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Regression shrinkage and selection via the lasso</article-title><source>J. Roy. Statist. Soc. B</source><year>1996</year><volume>58</volume><fpage>267</fpage><lpage>288</lpage></element-citation></ref><ref id="B27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>The lasso method for variable selection in the Cox model</article-title><source>Stat. Med</source><year>1997</year><volume>16</volume><fpage>385</fpage><lpage>395</lpage><pub-id pub-id-type="pmid">9044528</pub-id></element-citation></ref><ref id="B28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>P</given-names></name></person-group><article-title>Spatial smoothing and hot spot detection for CGH data using the fused lasso</article-title><source>Biostatistics</source><year>2007</year></element-citation></ref><ref id="B29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R</given-names></name><etal/></person-group><article-title>Sparsity and smoothness via the fused lasso</article-title><source>J. Roy. Statist. Soc. B</source><year>2005</year><volume>67</volume><fpage>91</fpage><lpage>108</lpage><comment>available at <ext-link ext-link-type="uri" xlink:href="http://ideas.repec.org/a/bla/jorssb/v67y2005i1p91-108.html">http://ideas.repec.org/a/bla/jorssb/v67y2005i1p91-108.html</ext-link>.</comment></element-citation></ref><ref id="B30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Trolet</surname><given-names>J</given-names></name><etal/></person-group><article-title>Genomic profiling and identification of high risk tumors in uveal melanoma by array-CGH analysis of primary tumors and liver metastases</article-title><source>submitted to Cancer Res</source><year>2008</year></element-citation></ref><ref id="B31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tschentscher</surname><given-names>F</given-names></name><etal/></person-group><article-title>Partial deletions of the long and short arm of chromosome 3 point to two tumor suppressor genes in uveal melanoma</article-title><source>Cancer Res</source><year>2001</year><volume>61</volume><fpage>3439</fpage><lpage>3442</lpage><pub-id pub-id-type="pmid">11309305</pub-id></element-citation></ref><ref id="B32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Beers</surname><given-names>E</given-names></name><name><surname>Nederlof</surname><given-names>P</given-names></name></person-group><article-title>Array-CGH and breast cancer</article-title><source>Breast Cancer Res</source><year>2006</year><volume>8</volume><fpage>210</fpage><pub-id pub-id-type="pmid">16817944</pub-id></element-citation></ref><ref id="B33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Roy</surname><given-names>N</given-names></name><etal/></person-group><article-title>Localization of the 17q breakpoint of a constitutional 1;17 translocation in a patient with neuroblastoma within a 25-kb segment located between the accn1 and tlk2 genes and near the distal breakpoints of two microdeletions in neurofibromatosis type 1 patients</article-title><source>Genes, Chromosomes Cancer</source><year>2002</year><volume>35</volume><fpage>113</fpage><lpage>120</lpage><pub-id pub-id-type="pmid">12203774</pub-id></element-citation></ref><ref id="B34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>VN</given-names></name></person-group><source>Statistical Learning Theory</source><year>1998</year><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="B35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waldman</surname><given-names>FM</given-names></name><etal/></person-group><article-title>Centromeric copy number of chromosome 7 is strongly correlated with tumor grade and labeling index in human bladder cancer</article-title><source>Cancer Res</source><year>1991</year><volume>51</volume><fpage>3807</fpage><lpage>3813</lpage><pub-id pub-id-type="pmid">1676611</pub-id></element-citation></ref><ref id="B36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>J</given-names></name><etal/></person-group><article-title>Combined cDNA Array Comparative Genomic Hybridization and Serial Analysis of Gene Expression Analysis of Breast Tumor Progression</article-title><source>Cancer Res</source><year>2006</year><volume>66</volume><fpage>4065</fpage><lpage>4078</lpage><pub-id pub-id-type="pmid">16618726</pub-id></element-citation></ref><ref id="B37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>J</given-names></name><etal/></person-group><person-group person-group-type="editor"><name><surname>Thrun</surname><given-names>S</given-names></name></person-group><article-title>1-norm support vector machines</article-title><source>Adv. Neural. Inform. Process Syst</source><year>2004</year><volume>volume 16</volume><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref></ref-list></back></article>